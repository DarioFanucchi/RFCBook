---
title: "Part IV — AutoML & H2O via modeltime.h2o"
---

# H2O & AutoML for forecasting

**Motivation**

Sometimes you want to search over many model classes automatically and get a strong baseline without hand–tuning. **H2O AutoML** is a mature AutoML system for tabular data. `modeltime.h2o` plugs it into the modeltime workflow, giving you AutoML–style model search while keeping time–series semantics in your resampling and features.

---

# Initialising H2O

```{r}
automl_available <- requireNamespace("modeltime.h2o", quietly = TRUE) &&
  requireNamespace("h2o", quietly = TRUE)

if (!automl_available) {
  message("modeltime.h2o + h2o not installed; AutoML chunks will be displayed but not evaluated.")
}
```

```{r, eval=automl_available}
require(h2o, quietly=TRUE)
require(modeltime.h2o, quietly=TRUE)

h2o.init()
```

You can control memory and cluster size via arguments to `h2o.init()` if needed.

---

# A basic AutoML forecasting workflow

We reuse the `data_x` example with `y`, `date`, `promo`, and `price`.

```{r, eval=automl_available}
splits_x <- time_series_split(
  data_x,
  assess     = 90,
  cumulative = TRUE
)

rec_h2o <- recipe(y ~ ., data = training(splits_x)) %>%
  step_timeseries_signature(date) %>%
  step_rm(
    date,
    contains("hour"),
    contains("minute"),
    contains("second"),
    contains("am.pm"),
    contains("lbl")
  ) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())
```

Specify an AutoML regression model:

```{r, eval=automl_available}
h2o_spec <- automl_reg(
  max_runtime_mins = 10,
  max_models       = 20,
  seed             = 123
) %>%
  set_engine("h2o")
```

Build workflow and fit:

```{r, eval=automl_available}
wf_h2o <- workflow() %>%
  add_recipe(rec_h2o) %>%
  add_model(h2o_spec)

fit_h2o <- wf_h2o %>%
  fit(training(splits_x))
```

Evaluate:

```{r, eval=automl_available}
modeltime_table(fit_h2o) %>%
  modeltime_calibrate(testing(splits_x)) %>%
  modeltime_accuracy()
```

Forecast:

```{r, eval=automl_available}
h2o_calib <- modeltime_table(fit_h2o) %>%
  modeltime_calibrate(testing(splits_x))

h2o_fc <- h2o_calib %>%
  modeltime_forecast(
    new_data    = testing(splits_x),
    actual_data = data_x
  )

h2o_fc
```

```{r, fig.cap="AutoML forecast (best H2O model) over the holdout window.", eval=automl_available}
h2o_fc %>%
  plot_modeltime_forecast(
    .title = "H2O AutoML leader vs actuals",
    .legend_max_width = 24
  )
```

---

# What H2O AutoML is doing under the hood

- Splits the training data internally into training/validation.
- Tries many model classes: GBM, Random Forest, GLM, deep learning, stacked ensembles, etc.
- Ranks them on a validation metric (e.g. RMSE or MAE).
- Returns a “leader” model that is exposed through the modeltime interface.

For deeper inspection, you can access the underlying H2O objects:

```{r, eval=automl_available}
leaderboard <- h2o.get_leaderboard()
leaderboard
```

---

# Time–series semantics & caveats

**Important**

H2O AutoML itself does **not** understand time. It sees a supervised regression problem. The time–series semantics come from:

- How you create features (lags, rolling stats, time signatures, promo/price variables).
- How you split and resample data (`time_series_split()`, `time_series_cv()`).

Guidelines:

- Always use **time–based splits** for evaluation.
- Prefer to handle leakage–sensitive operations (like scaling and lagging) via recipes or timetk, not inside AutoML.
- Be cautious with automatic random CV inside H2O; keep your main validation loop in your R/rsample layer.

---

# When to use H2O AutoML vs manual models

**Use AutoML when:**

- You want a strong baseline quickly.
- You’re exploring a new dataset and want to know what is achievable.
- You’re happy with a “black–box-ish” model bundle and care more about accuracy than fine control.

**Use manual models (fable/modeltime) when:**

- You want explicit control over model class and structure (e.g. ARIMAX vs XGBoost vs DeepAR).
- Interpretability and diagnostics matter (especially in trade/revenue discussions).
- You have custom constraints or loss functions that AutoML does not support.

In practice, a good pattern is:

1. Start with **fable** models and naive baselines for sanity.
2. Add **modeltime** global ML models (e.g. XGBoost / LightGBM) with well–engineered features.
3. Use **H2O AutoML** as an additional candidate in your modeltime ensembles.
4. Select the combination that works best across your backtests and business constraints.
