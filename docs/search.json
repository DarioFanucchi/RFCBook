[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical Forecasting in R with fable & modeltime",
    "section": "",
    "text": "This book is a compact, opinionated guide to modern time series forecasting in R, with a focus on:\n\nClassical, statistically principled methods using the fable ecosystem.\nMachine–learning–style forecasting using modeltime, tidymodels, and friends.\nPractical extensions for retail and supply–chain–flavoured data: promotions, prices, hierarchies, and global models.\nOptional AutoML and scalable modelling using H2O.\n\nThe style is example–led: every concept is tied to runnable R code.\nYou can read it straight through, or jump to the part that matches how you work today:\n\nPart I: fable\nPart II: modeltime + tidymodels\nPart III: Extensions (promos, hierarchies, feature engineering, global vs per–SKU)\nPart IV: AutoML with H2O",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Practical Forecasting in R with fable & modeltime</span>"
    ]
  },
  {
    "objectID": "part1-fable.html",
    "href": "part1-fable.html",
    "title": "2  Part I — Forecasting with fable",
    "section": "",
    "text": "3 Time series as tsibbles\nThis part of the book covers forecasting with traditional statistical methods. A great book covering these topics in depth is Forecasting: Principles and Practice by Rob Hyndman.\nMotivation\nThe fable ecosystem is built around the tsibble class: a tidy time–series structure with an explicit time index and (optionally) one or more keys. If your data is not in a tsibble, everything else becomes awkward.\nsuppressPackageStartupMessages(library(generics))\nsuppressPackageStartupMessages(library(tsibble))\nrequire(feasts, quietly = TRUE)\nrequire(fable, quietly = TRUE)\nrequire(fabletools, quietly = TRUE)\nrequire(ggplot2, quietly = TRUE)\nrequire(dplyr, quietly = TRUE)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:generics':\n\n    explain\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata &lt;- tsibble(\n  date  = as.Date(\"2020-01-01\") + 0:729,\n  y     = sin(2*pi*(0:729)/7) + 0.5*sin(2*pi*(0:729)/30.5) + rnorm(730, 0, 0.2), # Sample data with weekly, monthly trends\n  index = date\n)\n\ndata\n\n# A tsibble: 730 x 2 [1D]\n   date             y\n   &lt;date&gt;       &lt;dbl&gt;\n 1 2020-01-01 -0.412 \n 2 2020-01-02  0.867 \n 3 2020-01-03  1.23  \n 4 2020-01-04  0.815 \n 5 2020-01-05  0.0948\n 6 2020-01-06 -0.429 \n 7 2020-01-07 -0.397 \n 8 2020-01-08  0.518 \n 9 2020-01-09  1.13  \n10 2020-01-10  1.52  \n# ℹ 720 more rows\nYou now have a single daily series with 730 observations.\nautoplot(data, y) +\n  labs(\n    title = \"Toy demand signal\",\n    x = \"Date\",\n    y = \"y\"\n  ) + hiplot::theme_isazi(12)\n\n\n\n\nSimulated daily series showing weekly seasonality.\nWeekly pulses sit on top of a slower monthly undulation—use the plot to confirm the simulated data actually contains the structure you plan to model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part I — Forecasting with fable</span>"
    ]
  },
  {
    "objectID": "part1-fable.html#conceptual-view",
    "href": "part1-fable.html#conceptual-view",
    "title": "2  Part I — Forecasting with fable",
    "section": "11.1 Conceptual view",
    "text": "11.1 Conceptual view\nStack all series at time t into a vector yₜ. There exists a summing matrix S such that\n\\[\n\\mathbf{y}_t = \\mathbf{S} \\, \\mathbf{b}_t\n\\]\nwhere bₜ are the bottom–level series (e.g., SKUs).\nIf you fit arbitrary models to each series, you obtain base forecasts \\(\\hat{\\mathbf{y}}_h\\) that in general are not coherent.\nReconciliation finds adjusted forecasts\n\\[\n\\tilde{\\mathbf{y}}_h = \\mathbf{S} \\, \\mathbf{P} \\, \\hat{\\mathbf{y}}^{[b]}_h\n\\]\nsuch that:\n\nCoherence holds by construction (aggregation uses S).\nThe adjustments are optimal according to some criterion.\n\nMinT (“Minimum Trace”) chooses P to minimise the trace of the reconciled error covariance, using an estimate of the base forecast error covariance matrix W.\nIn one common parameterisation:\n\\[\n\\tilde{\\mathbf{y}}_h = \\mathbf{S} (\\mathbf{S}^\\top \\mathbf{W}^{-1} \\mathbf{S})^{-1} \\mathbf{S}^\\top \\mathbf{W}^{-1} \\hat{\\mathbf{y}}_h\n\\]\nIntuition:\n\nSeries with high variance forecasts get shrunk more towards coherent aggregates.\nSeries whose errors are highly correlated with others share information more strongly.\n\nIn practice, fable estimates W from in–sample residuals and performs this matrix algebra for you.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part I — Forecasting with fable</span>"
    ]
  },
  {
    "objectID": "part1-fable.html#using-reconciliation-in-fable",
    "href": "part1-fable.html#using-reconciliation-in-fable",
    "title": "2  Part I — Forecasting with fable",
    "section": "11.2 Using reconciliation in fable",
    "text": "11.2 Using reconciliation in fable\n\nrequire(fabletools, quietly = TRUE)\nrequire(tsibbledata, quietly = TRUE)\n\ntourism &lt;- tourism  # key = (Region, Purpose)\ntourism_small &lt;- tourism %&gt;%\n  filter(\n    Region %in% c(\"Sydney\", \"Melbourne\", \"Brisbane\", \"Perth\"),\n    Purpose %in% c(\"Business\", \"Holiday\")\n  )\n\nfit &lt;- tourism_small %&gt;%\n  model(ets = ETS(Trips))\n\nrec &lt;- fit %&gt;%\n  reconcile(\n    bu = bottom_up(ets)\n  )\n\nfc &lt;- rec %&gt;%\n  forecast(h = \"3 years\")\n\nGuidelines:\n\nbottom_up(): use when bottom–level series are well measured and you care most about them.\nmin_trace() / mint_shrink: use when you want balanced accuracy across levels, and when noise at the bottom is substantial.\n(Omitted from the runnable example for speed, but you can add min_trace(ets) or min_trace(ets, method = \"mint_shrink\") in the reconcile() call above to try MinT.)\n\n\nfc %&gt;%\n  filter(Region == \"Sydney\", Purpose == \"Holiday\") %&gt;%\n  autoplot(\n    tourism_small %&gt;%\n      filter(Region == \"Sydney\", Purpose == \"Holiday\")\n  ) +\n  labs(\n    title = \"Sydney holiday forecasts: base ETS vs bottom-up\",\n    x = \"Year\",\n    y = \"Trips\"\n  ) + \n  hiplot::theme_isazi(12)\n\n\n\n\nReconciled forecasts for Sydney holiday travel across reconciliation strategies.\n\n\n\n\nBottom-up coherence forces the reconciled line (orange) to track the aggregate closely; if the adjustment is too aggressive, try MinT with shrinkage to balance bottom vs top accuracy.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part I — Forecasting with fable</span>"
    ]
  },
  {
    "objectID": "part2-modeltime.html",
    "href": "part2-modeltime.html",
    "title": "3  Part II — Forecasting with modeltime & tidymodels",
    "section": "",
    "text": "4 Time series splits & the ML mindset\nMotivation\nMachine–learning–style forecasting treats time series as supervised learning with strong temporal structure. The critical decision is the evaluation protocol; you must respect time when you split data.\n\nrequire(dplyr, quietly=TRUE)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nrequire(modeltime, quietly=TRUE)\ntime_series_split &lt;- timetk::time_series_split\ntime_series_cv &lt;- timetk::time_series_cv\n\ndata &lt;- tibble::tibble(\n  date  = as.Date(\"2020-01-01\") + 0:729,\n  y     = sin(2*pi*(0:729)/7) + 2*sin(2*pi*(0:729)/30.5) + sin(2*pi*(0:729)/365) + rnorm(730, 0, 0.2), # Sample data with weekly, monthly, yearly trends\n)\n\nsplits &lt;- time_series_split(\n  data,\n  assess     = 90,\n  cumulative = TRUE\n)\n\nUsing date_var: date\n\n\n\ntraining(splits) gives history up to the cutoff.\ntesting(splits) gives the final 90 days for evaluation.\n\n\n\n\n5 A basic XGBoost forecast\nMotivation\nGradient–boosted trees (XGBoost, LightGBM, CatBoost) are state of the art in many retail forecasting benchmarks when combined with good feature engineering.\n\nrec &lt;- recipes::recipe(y ~ date, data = rsample::training(splits)) %&gt;%\n  timetk::step_timeseries_signature(date) %&gt;%\n  recipes::step_rm(\n    date,\n    tidyselect::contains(\"hour\"),\n    tidyselect::contains(\"minute\"),\n    tidyselect::contains(\"second\"),\n    tidyselect::contains(\"am.pm\"),\n    tidyselect::contains(\"lbl\")\n  ) %&gt;%\n  recipes::step_zv(recipes::all_predictors()) %&gt;%\n  recipes::step_normalize(recipes::all_numeric_predictors())\n\nxgb_spec &lt;- parsnip::boost_tree(\n  trees      = 2000,\n  learn_rate = 0.03,\n  tree_depth = 8\n) %&gt;%\n  parsnip::set_engine(\"xgboost\") %&gt;%\n  parsnip::set_mode(\"regression\")\n\nwf_xgb &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec) %&gt;%\n  workflows::add_model(xgb_spec)\n\nfit_xgb &lt;- workflows::fit(wf_xgb, rsample::training(splits))\n\nForecast and evaluate:\n\nxgb_tbl &lt;- modeltime::modeltime_table(fit_xgb)\n\nxgb_calib &lt;- xgb_tbl %&gt;%\n  modeltime::modeltime_calibrate(rsample::testing(splits))\n\nxgb_fc &lt;- xgb_calib %&gt;%\n  modeltime::modeltime_forecast(\n    new_data    = rsample::testing(splits),\n    actual_data = data\n  )\n\nxgb_fc\n\n# Forecast Results\n  \n\n\nConf Method: conformal_default | Conf Interval: 0.95 | Conf By ID: FALSE\n(GLOBAL CONFIDENCE)\n\n\n# A tibble: 820 × 7\n   .model_id .model_desc .key   .index     .value .conf_lo .conf_hi\n       &lt;int&gt; &lt;chr&gt;       &lt;fct&gt;  &lt;date&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1        NA ACTUAL      actual 2020-01-01 -0.216       NA       NA\n 2        NA ACTUAL      actual 2020-01-02  1.43        NA       NA\n 3        NA ACTUAL      actual 2020-01-03  1.89        NA       NA\n 4        NA ACTUAL      actual 2020-01-04  1.69        NA       NA\n 5        NA ACTUAL      actual 2020-01-05  0.996       NA       NA\n 6        NA ACTUAL      actual 2020-01-06  0.897       NA       NA\n 7        NA ACTUAL      actual 2020-01-07  1.28        NA       NA\n 8        NA ACTUAL      actual 2020-01-08  2.30        NA       NA\n 9        NA ACTUAL      actual 2020-01-09  2.88        NA       NA\n10        NA ACTUAL      actual 2020-01-10  3.17        NA       NA\n# ℹ 810 more rows\n\n\n\nxgb_fc %&gt;%\n  modeltime::plot_modeltime_forecast(\n    .title = \"XGBoost forecast vs actuals\",\n    .legend_max_width = 24\n  )\n\n\n\nHoldout forecast from the basic XGBoost workflow.\n\n\nLook for systematic under/over prediction across the holdout—if the orange forecast line consistently lags the truth, feed more lags or external drivers before chasing hyperparameters.\n\n\n\n6 Adding a classical ARIMA baseline\nMotivation\nEven when you believe ML will win, you should verify it beats a decent statistical model.\n\nrec_arima &lt;- recipes::recipe(y ~ date, data = rsample::training(splits))\n\nfit_arima &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec_arima) %&gt;%\n  workflows::add_model(\n    modeltime::arima_reg() %&gt;%\n      parsnip::set_engine(\"auto_arima\")\n  ) %&gt;%\n  workflows::fit(rsample::training(splits))\n\nfrequency = 7 observations per 1 week\n\nmodel_tbl &lt;- modeltime::modeltime_table(\n  fit_xgb,\n  fit_arima\n)\n\naccuracy_tbl &lt;- model_tbl %&gt;%\n  modeltime::modeltime_calibrate(rsample::testing(splits)) %&gt;%\n  modeltime::modeltime_accuracy()\n\naccuracy_tbl\n\n# A tibble: 2 × 9\n  .model_id .model_desc                .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 XGBOOST                    Test  0.375  60.5 0.586  49.2 0.457 0.917\n2         2 ARIMA(1,0,0)(2,1,0)[7] WI… Test  1.77  460.  2.76  110.  2.19  0.215\n\n\n\naccuracy_tbl %&gt;%\n  ggplot2::ggplot(ggplot2::aes(x = reorder(.model_desc, rmse), y = rmse)) +\n  ggplot2::geom_col(fill = \"#3182bd\") +\n  ggplot2::coord_flip() +\n  ggplot2::labs(\n    x = NULL,\n    y = \"RMSE\",\n    title = \"Holdout accuracy by model\"\n  ) +\n  hiplot::theme_isazi(12)\n\n\n\n\nARIMA offers a sturdy baseline; XGBoost wins only if the engineered features add signal.\n\n\n\n\nYou now have side–by–side accuracy for ML and classical methods.\nEven if RMSE numbers look close, the bar plot highlights whether the ML lift is material enough to justify the extra complexity.\n\n\n\n7 Exogenous regressors: promos, prices, features\nMotivation\nMachine–learning models handle many regressors naturally. This is key for retail and supply–chain: promotions, prices, macro variables, calendar features, etc.\n\nset.seed(1)\n\ndata_x &lt;- data %&gt;%\n  mutate(\n    promo = rbinom(n(), 1, 0.1),\n    price = runif(n(), 9, 12)\n  )\n\nsplits_x &lt;- time_series_split(\n  data_x,\n  assess     = 90,\n  cumulative = TRUE\n)\n\nUsing date_var: date\n\nrec_x &lt;- recipes::recipe(y ~ ., data = rsample::training(splits_x)) %&gt;%\n  timetk::step_timeseries_signature(date) %&gt;%\n  recipes::step_rm(\n    date,\n    tidyselect::contains(\"hour\"),\n    tidyselect::contains(\"minute\"),\n    tidyselect::contains(\"second\"),\n    tidyselect::contains(\"am.pm\"),\n    tidyselect::contains(\"lbl\")\n  ) %&gt;%\n  recipes::step_zv(recipes::all_predictors()) %&gt;%\n  recipes::step_normalize(recipes::all_numeric_predictors())\n\nwf_xgb_x &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec_x) %&gt;%\n  workflows::add_model(xgb_spec)\n\nfit_xgb_x &lt;- workflows::fit(wf_xgb_x, rsample::training(splits_x))\n\nYou can interpret feature importance with packages like vip, DALEX, or iml.\nUse this when you want flexible, global promo/price effects; if you need per-series coefficients, see the ARIMAX treatment in Part I, and for cross-series engineered drivers jump ahead to Part III.\n\nmodeltime::modeltime_table(fit_xgb_x) %&gt;%\n  modeltime::modeltime_calibrate(rsample::testing(splits_x)) %&gt;%\n  modeltime::modeltime_forecast(\n    new_data    = rsample::testing(splits_x),\n    actual_data = data_x\n  ) %&gt;%\n  modeltime::plot_modeltime_forecast(\n    .title = \"Holdout forecast with promo/price features\",\n    .legend_max_width = 24\n  )\n\n\n\nPromo + price features bend the forecast line upward when uplift signals appear in the training data.\n\n\nNotice whether spikes during promo periods are now captured; if not, increase lag depth or add richer promo descriptors.\n\n\n\n8 Time–aware hyperparameter tuning\nMotivation\nIf you tune XGBoost with random cross–validation, you leak future information. Instead, use time–series CV (rolling origin) for tuning.\n\ncv_folds &lt;- time_series_cv(\n  data_x,\n  assess     = 90,\n  skip       = 30,\n  cumulative = TRUE\n)\n\nUsing date_var: date\n\ntuned &lt;- tune::tune_grid(\n  wf_xgb_x,\n  resamples = cv_folds,\n  grid      = 20,\n  control   = tune::control_grid(verbose = FALSE)\n)\n\nWarning: No tuning parameters have been detected, performance will be evaluated using\nthe resamples with no tuning.\nDid you want to assign any parameters with a value of `tune()`?\n\nbest &lt;- tune::select_best(tuned, metric = \"rmse\")\n\nfinal_wf &lt;- tune::finalize_workflow(wf_xgb_x, best)\nfit_final &lt;- workflows::fit(final_wf, data_x)\n\nThe tuned workflow can then be used in modeltime_table() and productionised.\n\n\n\n9 Ensembles in modeltime\nMotivation\nEnsembling multiple models (ML + classical) often yields robustness and extra accuracy at very low marginal cost.\n\nmodel_tbl &lt;- modeltime::modeltime_table(\n  fit_xgb_x,\n  fit_arima\n)\n\nensemble &lt;- model_tbl %&gt;%\n  modeltime.ensemble::ensemble_average(type = \"mean\")\n\nensemble %&gt;%\n  modeltime::modeltime_calibrate(rsample::testing(splits_x)) %&gt;%\n  modeltime::modeltime_accuracy()\n\nConverting to Modeltime Table.\n\n\n# A tibble: 1 × 9\n  .model_id .model_desc               .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 ENSEMBLE (MEAN): 2 MODELS Test  0.946  240.  1.48  97.0  1.18 0.843\n\nensemble_fc &lt;- ensemble %&gt;%\n  modeltime::modeltime_calibrate(rsample::testing(splits_x)) %&gt;%\n  modeltime::modeltime_forecast(\n    new_data    = rsample::testing(splits_x),\n    actual_data = data_x\n  )\n\nConverting to Modeltime Table.\n\n\n\nensemble_fc %&gt;%\n  modeltime::plot_modeltime_forecast(\n    .title = \"Ensemble forecast vs truth\",\n    .legend_max_width = 24\n  )\n\n\n\nSimple mean ensemble of ML + ARIMA across the holdout window.\n\n\nYou can also use weighted ensembles or meta–learners via stacks.\nThe blended line often inherits the best bits of each contributor; if it still wobbles, inspect residual correlations to design smarter weighting schemes.\n\n\n\n10 Global models across many series\nMotivation\nGlobal models train one model across many series (e.g. SKUs, depots, stores), often outperforming per–series models when data is sparse.\n\ndata_panel &lt;- tibble::tibble(\n  sku   = rep(letters[1:20], each = 730),\n  date  = rep(as.Date(\"2020-01-01\") + 0:729, 20),\n  y     = rnorm(20 * 730) + as.numeric(as.factor(sku)) * 0.2,\n  promo = rbinom(20 * 730, 1, 0.1),\n  price = runif(20 * 730, 9, 12)\n) %&gt;%\n  arrange(date, sku)\n\nsplits_panel &lt;- time_series_split(\n  data_panel,\n  assess     = 90,\n  cumulative = TRUE\n)\n\nUsing date_var: date\n\n\nOverlapping Timestamps Detected. Processing overlapping time series together using sliding windows.\n\nrec_panel &lt;- recipes::recipe(y ~ sku + date + promo + price,\n                    data = rsample::training(splits_panel)) %&gt;%\n  timetk::step_timeseries_signature(date) %&gt;%\n  recipes::step_rm(date) %&gt;%\n  recipes::step_dummy(recipes::all_nominal_predictors()) %&gt;%\n  recipes::step_zv(recipes::all_predictors()) %&gt;%\n  recipes::step_normalize(recipes::all_numeric_predictors())\n\nwf_global &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec_panel) %&gt;%\n  workflows::add_model(xgb_spec)\n\nfit_global &lt;- workflows::fit(wf_global, rsample::training(splits_panel))\n\nThis XGBoost model learns patterns shared across SKUs and depots.\n\n\n\n11 Prophet in modeltime\nMotivation\nProphet is a popular model for business time series with strong seasonal patterns and holiday effects. modeltime wraps it in the same interface as your other models.\n\nrequire(prophet, quietly=TRUE)\n\nprop_spec &lt;- modeltime::prophet_reg(\n  seasonality_yearly = TRUE,\n  seasonality_weekly = TRUE,\n  seasonality_daily  = FALSE,\n  changepoint_num    = 25,\n  changepoint_range  = 0.9\n) %&gt;%\n  parsnip::set_engine(\"prophet\")\n\nrec_prophet &lt;- recipes::recipe(y ~ date, data = rsample::training(splits))  # Prophet needs date and y\n\nwf_prophet &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec_prophet) %&gt;%\n  workflows::add_model(prop_spec)\n\nfit_prophet &lt;- workflows::fit(wf_prophet, rsample::training(splits))\n\nProphet with extra regressors:\n\nrec_prophet_x &lt;- recipes::recipe(y ~ date + promo + price,\n                        data = rsample::training(splits_x)) %&gt;%\n  recipes::step_mutate(\n    promo = as.numeric(promo),\n    price = price\n  )\n\nfit_prophet_x &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec_prophet_x) %&gt;%\n  workflows::add_model(prop_spec) %&gt;%\n  workflows::fit(rsample::training(splits_x))\n\nBoosted Prophet (prophet_boost()) adds a gradient–boosting component on Prophet’s residuals; the interface is analogous, with set_engine(\"prophet_xgboost\").",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part II — Forecasting with modeltime & tidymodels</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html",
    "href": "part3-extensions.html",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "",
    "text": "5 Advanced timetk usage\nMotivation\ntimetk is a powerful toolkit for time–based feature engineering, visualisation, and cross–validation. It plays very nicely with both tidymodels and direct data–frame workflows.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#time-series-signatures",
    "href": "part3-extensions.html#time-series-signatures",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "5.1 Time series signatures",
    "text": "5.1 Time series signatures\nYou can generate time–based features directly, without recipes:\n\nrequire(dplyr, quietly = TRUE)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nrequire(ggplot2, quietly = TRUE)\ntime_series_split &lt;- timetk::time_series_split\ntime_series_cv &lt;- timetk::time_series_cv\n\nxgb_spec &lt;- parsnip::boost_tree(\n  trees = 1000,\n  learn_rate = 0.03,\n  tree_depth = 8\n) %&gt;%\n  parsnip::set_engine(\"xgboost\") %&gt;%\n  parsnip::set_mode(\"regression\")\n\ndata &lt;- tibble::tibble(\n  date = as.Date(\"2020-01-01\") + 0:729,\n  y    = sin(2*pi*(0:729)/7) + rnorm(730, 0, 0.2)\n)\n\nrec &lt;- recipes::recipe(y ~ date, data = data)\n\ndata_sig &lt;- data %&gt;%\n  timetk::tk_augment_timeseries_signature(date) %&gt;%\n  select(-tidyselect::contains(\"hour\"), -tidyselect::contains(\"minute\"),\n         -tidyselect::contains(\"second\"), -tidyselect::contains(\"am.pm\"))\n\nThis yields fields like year, month, week, day of week, quarter, etc.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#lags-and-sliding-windows",
    "href": "part3-extensions.html#lags-and-sliding-windows",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "5.2 Lags and sliding windows",
    "text": "5.2 Lags and sliding windows\n\ndata_lags &lt;- data_sig %&gt;%\n  timetk::tk_augment_lags(y, .lags = c(1, 7, 14))\n\ndata_roll &lt;- data_lags %&gt;%\n  timetk::tk_augment_slidify(\n    .value  = y,\n    .f      = ~ mean(.x, na.rm = TRUE),\n    .period = 7,\n    .align  = \"right\",\n    .partial = FALSE,\n    .names  = \"roll_mean_7\"\n  )\n\nWarning: `cross_df()` was deprecated in purrr 1.0.0.\nℹ Please use `tidyr::expand_grid()` instead.\nℹ See &lt;https://github.com/tidyverse/purrr/issues/768&gt;.\nℹ The deprecated feature was likely used in the timetk package.\n  Please report the issue at\n  &lt;https://github.com/business-science/timetk/issues&gt;.\n\n\nWarning: `cross()` was deprecated in purrr 1.0.0.\nℹ Please use `tidyr::expand_grid()` instead.\nℹ See &lt;https://github.com/tidyverse/purrr/issues/768&gt;.\nℹ The deprecated feature was likely used in the purrr package.\n  Please report the issue at &lt;https://github.com/tidyverse/purrr/issues&gt;.\n\n\nThis produces lagged versions of y and a rolling 7–day mean.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#visualising-timeseries-cv-plans",
    "href": "part3-extensions.html#visualising-timeseries-cv-plans",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "5.3 Visualising time–series CV plans",
    "text": "5.3 Visualising time–series CV plans\n\ncv_plan &lt;- time_series_cv(\n  data,\n  assess     = 90,\n  skip       = 30,\n  cumulative = TRUE\n)\n\nUsing date_var: date\n\ncv_plan_tbl &lt;- cv_plan %&gt;%\n  timetk::tk_time_series_cv_plan() %&gt;%\n  mutate(slice = factor(.id, levels = rev(unique(.id))))\n\nggplot(cv_plan_tbl, aes(x = date, y = slice, fill = .key)) +\n  geom_tile(height = 0.6, colour = \"white\") +\n  scale_fill_manual(values = c(training = \"#2b8cbe\", testing = \"#f03b20\")) +\n  labs(\n    title = \"Rolling CV plan: train (blue) vs assessment (orange)\",\n    x = \"Date\",\n    y = \"Slice\",\n    fill = NULL\n  ) +\n  hiplot::theme_isazi(12)\n\n\n\n\n\n\n\n\nThis allows you to inspect your rolling window splits visually and confirm they make sense.\nBlue blocks show the expanding training windows while orange shows the held-out horizon; scan horizontally to confirm the assessment windows march forward without gaps or overlaps.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#arimax-in-fable-for-promo-price",
    "href": "part3-extensions.html#arimax-in-fable-for-promo-price",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "6.1 ARIMAX in fable for promo & price",
    "text": "6.1 ARIMAX in fable for promo & price\n\ndata_retail &lt;- tsibble::tsibble(\n  date   = as.Date(\"2020-01-01\") + 0:729,\n  sales  = rpois(730, 100),\n  promo  = rbinom(730, 1, 0.1),\n  price  = runif(730, 9, 12),\n  index  = date\n)\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\nfit_arimax &lt;- data_retail %&gt;%\n  fabletools::model(\n    arimax = fable::ARIMA(sales ~ promo + price)\n  )\n\nfit_arimax %&gt;% fabletools::report()\n\nSeries: sales \nModel: LM w/ ARIMA(0,0,3) errors \n\nCoefficients:\n         ma1     ma2      ma3   promo   price  intercept\n      0.0191  0.0183  -0.1595  0.9060  0.1561    97.9842\ns.e.  0.0368  0.0378   0.0360  1.1423  0.4246     4.4846\n\nsigma^2 estimated as 96.24:  log likelihood=-2699.76\nAIC=5413.53   AICc=5413.68   BIC=5445.68\n\n\nInterpretation:\n\nCoefficient on promo ≈ average incremental units during promo.\nCoefficient on price ≈ price elasticity (units per price unit); often negative.\n\nYou can add lags and interactions, e.g. promo + lag(promo) + price + promo:holiday_flag.\n\nfit_arimax %&gt;%\n  fabletools::augment() %&gt;%\n  dplyr::left_join(data_retail %&gt;% dplyr::select(date, promo), by = \"date\") %&gt;%\n  mutate(promo_flag = if_else(promo == 1, \"Promo\", \"Non-promo\")) %&gt;%\n  ggplot(aes(x = date)) +\n  geom_line(aes(y = sales), colour = \"grey70\") +\n  geom_line(aes(y = .fitted), colour = \"#2c7fb8\", linewidth = 0.6) +\n  geom_point(aes(y = sales, colour = promo_flag), alpha = 0.7, size = 1.1) +\n  scale_colour_manual(values = c(\"Promo\" = \"#d95f02\", \"Non-promo\" = \"#1b9e77\")) +\n  labs(\n    title = \"Promo vs non-promo fit diagnostics\",\n    x = \"Date\",\n    y = \"Sales\",\n    colour = NULL\n  ) +\n  hiplot::theme_isazi(12)\n\n\n\n\nPromo periods (orange) sit well above the fitted baseline, showing clean incremental lift.\n\n\n\n\nIf the promo dots mingle with the baseline, consider adding lagged promos or interaction terms (promo × season) before discarding the regressor.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#ml-global-model-for-promos-price",
    "href": "part3-extensions.html#ml-global-model-for-promos-price",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "6.2 ML global model for promos & price",
    "text": "6.2 ML global model for promos & price\n\ndata_panel &lt;- tibble(\n  sku   = rep(letters[1:20], each = 730),\n  date  = rep(as.Date(\"2020-01-01\") + 0:729, 20),\n  sales = rpois(20 * 730, 100),\n  promo = rbinom(20 * 730, 1, 0.1),\n  price = runif(20 * 730, 9, 12)\n) %&gt;%\n  arrange(date, sku)\n\nsplits_p &lt;- time_series_split(\n  data_panel,\n  assess     = 90,\n  cumulative = TRUE\n)\n\nUsing date_var: date\n\n\nOverlapping Timestamps Detected. Processing overlapping time series together using sliding windows.\n\nrec_promos &lt;- recipes::recipe(sales ~ sku + date + promo + price,\n                     data = rsample::training(splits_p)) %&gt;%\n  timetk::step_timeseries_signature(date) %&gt;%\n  recipes::step_rm(date) %&gt;%\n  recipes::step_dummy(recipes::all_nominal_predictors(), one_hot = TRUE) %&gt;%\n  recipes::step_zv(recipes::all_predictors()) %&gt;%\n  recipes::step_normalize(recipes::all_numeric_predictors())\n\nwf_promos &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec_promos) %&gt;%\n  workflows::add_model(xgb_spec)\n\nfit_promos &lt;- workflows::fit(wf_promos, rsample::training(splits_p))\n\nFor cannibalisation, you precompute features such as:\n\ncategory_sales_ex_sku\nbrand_sales\ncompetitor_price\n\nvia group–by / lag operations, then feed into the recipe.\n\nrequire(vip, quietly = TRUE)\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nfit_promos %&gt;%\n  workflows::pull_workflow_fit() %&gt;%\n  vip::vip(num_features = 10, geom = \"col\") +\n  hiplot::theme_isazi(12)\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nℹ Please use `extract_fit_parsnip()` instead.\n\n\n\n\n\nGlobal XGBoost importance shows which levers the model leaned on first.\n\n\n\n\nPromo and price should dominate; if only date-derived factors appear, revisit how you encoded the commercial drivers.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#timebased-features",
    "href": "part3-extensions.html#timebased-features",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "7.1 Time–based features",
    "text": "7.1 Time–based features\n\nstep_timeseries_signature(date) or tk_augment_timeseries_signature().\nFourier terms for long seasonalities (e.g. yearly with daily data).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#lags-rolling-statistics",
    "href": "part3-extensions.html#lags-rolling-statistics",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "7.2 Lags & rolling statistics",
    "text": "7.2 Lags & rolling statistics\nIn recipes:\n\nrec_lags &lt;- rec %&gt;%\n  recipes::step_lag(y, lag = c(1, 7, 14)) %&gt;%\n  timetk::step_slidify(\n    y,\n    period = 7,\n    .f = mean,\n    align = \"right\",\n    partial = FALSE,\n    names = \"roll_mean_7\"\n  ) %&gt;%\n  timetk::step_slidify(\n    y,\n    period = 7,\n    .f = sd,\n    align = \"right\",\n    partial = FALSE,\n    names = \"roll_sd_7\"\n  )\n\nThese capture local dynamics that trees exploit well.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#calendar-holiday-features",
    "href": "part3-extensions.html#calendar-holiday-features",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "7.3 Calendar & holiday features",
    "text": "7.3 Calendar & holiday features\nUse step_holiday() or precomputed holiday tables to add dummies for important days, long weekends, etc.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#price-promoderived-features",
    "href": "part3-extensions.html#price-promoderived-features",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "7.4 Price & promo–derived features",
    "text": "7.4 Price & promo–derived features\nExamples:\n\ndiscount_pct = pmax(0, (list_price - price) / list_price)\npromo_flag = as.integer(discount_pct &gt; 0.1)\nprice_index vs category average.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#crossseries-interaction-features",
    "href": "part3-extensions.html#crossseries-interaction-features",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "7.5 Cross–series interaction features",
    "text": "7.5 Cross–series interaction features\nUse group–by / summarise to compute, for each date and category:\n\ncategory_sales\n\ncategory_sales_ex_sku\n\ntop_brand_share\n\nThen join back to the main table and feed into your recipe.\n\ncross_feats &lt;- data_panel %&gt;%\n  mutate(category = if_else(sku %in% letters[1:10], \"A\", \"B\")) %&gt;%\n  group_by(date, category) %&gt;%\n  mutate(\n    category_sales = sum(sales),\n    category_sales_ex_sku = category_sales - sales\n  ) %&gt;%\n  ungroup()\n\ncross_feats %&gt;%\n  select(sku, date, sales, category, category_sales_ex_sku) %&gt;%\n  arrange(sku, date) %&gt;%\n  head()\n\n# A tibble: 6 × 5\n  sku   date       sales category category_sales_ex_sku\n  &lt;chr&gt; &lt;date&gt;     &lt;int&gt; &lt;chr&gt;                    &lt;int&gt;\n1 a     2020-01-01   116 A                          950\n2 a     2020-01-02   110 A                          882\n3 a     2020-01-03    76 A                          934\n4 a     2020-01-04   103 A                          910\n5 a     2020-01-05   101 A                          900\n6 a     2020-01-06   111 A                          943\n\n\nThis pattern lets each SKU know the scale of its peers, unlocking cannibalisation or halo effects for downstream models.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#persku-arima-fable",
    "href": "part3-extensions.html#persku-arima-fable",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "8.1 Per–SKU ARIMA (fable)",
    "text": "8.1 Per–SKU ARIMA (fable)\n\nts_panel &lt;- tsibble::as_tsibble(\n  data_panel,\n  key   = sku,\n  index = date\n)\n\nsplit_date &lt;- as.Date(\"2021-06-30\")\n\ntrain_ts &lt;- ts_panel %&gt;%\n  filter(date &lt;= split_date)\n\ntest_ts &lt;- ts_panel %&gt;%\n  filter(date &gt; split_date)\n\nfit_per &lt;- train_ts %&gt;%\n  fabletools::model(arima = fable::ARIMA(sales))\n\nWarning in sqrt(diag(best$var.coef)): NaNs produced\n\nacc_per &lt;- fc_per &lt;- fit_per %&gt;%\n  fabletools::forecast(h = dplyr::n_distinct(test_ts$date)) %&gt;%\n  as_tibble() %&gt;%\n  transmute(\n    sku,\n    date,\n    .pred = purrr::map_dbl(sales, mean)\n  ) %&gt;%\n  left_join(\n    tsibble::as_tibble(test_ts) %&gt;%\n      select(sku, date, sales),\n    by = c(\"sku\", \"date\")\n  ) %&gt;%\n  group_by(sku) %&gt;%\n  summarise(\n    rmse_per = yardstick::rmse_vec(truth = sales, estimate = .pred),\n    .groups = \"drop\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part3-extensions.html#global-xgboost-modeltime",
    "href": "part3-extensions.html#global-xgboost-modeltime",
    "title": "4  Part III — Extensions, retail patterns & feature engineering",
    "section": "8.2 Global XGBoost (modeltime)",
    "text": "8.2 Global XGBoost (modeltime)\n\nsplits_panel &lt;- time_series_split(\n  data_panel,\n  assess     = 600,\n  cumulative = TRUE\n)\n\nUsing date_var: date\n\n\nOverlapping Timestamps Detected. Processing overlapping time series together using sliding windows.\n\nrec_global &lt;- recipes::recipe(sales ~ sku + date + promo + price,\n                     data = rsample::training(splits_panel)) %&gt;%\n  timetk::step_timeseries_signature(date) %&gt;%\n  recipes::step_rm(date) %&gt;%\n  recipes::step_dummy(recipes::all_nominal_predictors(), one_hot = TRUE) %&gt;%\n  recipes::step_zv(recipes::all_predictors()) %&gt;%\n  recipes::step_normalize(recipes::all_numeric_predictors())\n\nwf_global &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec_global) %&gt;%\n  workflows::add_model(xgb_spec)\n\nfit_global &lt;- workflows::fit(wf_global, rsample::training(splits_panel))\n\nfc_global &lt;- modeltime::modeltime_table(fit_global) %&gt;%\n  modeltime::modeltime_calibrate(rsample::testing(splits_panel)) %&gt;%\n  modeltime::modeltime_forecast(\n    new_data    = rsample::testing(splits_panel),\n    actual_data = data_panel,\n    keep_data   = TRUE\n  )\n\nMap to per–SKU RMSE:\n\nacc_global &lt;- fc_global %&gt;%\n  filter(.key == \"prediction\") %&gt;%\n  group_by(sku) %&gt;%\n  summarise(\n    rmse_global = yardstick::rmse_vec(truth = sales, estimate = .value),\n    .groups = \"drop\"\n  )\n\ncomparison &lt;- acc_per %&gt;%\n  left_join(acc_global, by = \"sku\") %&gt;%\n  mutate(diff = rmse_per - rmse_global)\n\nYou can now see how many SKUs prefer global vs per–SKU models and by how much.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part III — Extensions, retail patterns & feature engineering</span>"
    ]
  },
  {
    "objectID": "part4-automl-h2o.html",
    "href": "part4-automl-h2o.html",
    "title": "5  Part IV — AutoML & H2O via modeltime.h2o",
    "section": "",
    "text": "6 H2O & AutoML for forecasting\nMotivation\nSometimes you want to search over many model classes automatically and get a strong baseline without hand–tuning. H2O AutoML is a mature AutoML system for tabular data. modeltime.h2o plugs it into the modeltime workflow, giving you AutoML–style model search while keeping time–series semantics in your resampling and features.\n\n\n\n7 Initialising H2O\n\nautoml_available &lt;- requireNamespace(\"modeltime.h2o\", quietly = TRUE) &&\n  requireNamespace(\"h2o\", quietly = TRUE)\n\nif (!automl_available) {\n  message(\"modeltime.h2o + h2o not installed; AutoML chunks will be displayed but not evaluated.\")\n}\n\nmodeltime.h2o + h2o not installed; AutoML chunks will be displayed but not evaluated.\n\n\n\nrequire(h2o, quietly=TRUE)\nrequire(modeltime.h2o, quietly=TRUE)\n\nh2o.init()\n\nYou can control memory and cluster size via arguments to h2o.init() if needed.\n\n\n\n8 A basic AutoML forecasting workflow\nWe reuse the data_x example with y, date, promo, and price.\n\nsplits_x &lt;- time_series_split(\n  data_x,\n  assess     = 90,\n  cumulative = TRUE\n)\n\nrec_h2o &lt;- recipe(y ~ ., data = training(splits_x)) %&gt;%\n  step_timeseries_signature(date) %&gt;%\n  step_rm(\n    date,\n    contains(\"hour\"),\n    contains(\"minute\"),\n    contains(\"second\"),\n    contains(\"am.pm\"),\n    contains(\"lbl\")\n  ) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nSpecify an AutoML regression model:\n\nh2o_spec &lt;- automl_reg(\n  max_runtime_mins = 10,\n  max_models       = 20,\n  seed             = 123\n) %&gt;%\n  set_engine(\"h2o\")\n\nBuild workflow and fit:\n\nwf_h2o &lt;- workflow() %&gt;%\n  add_recipe(rec_h2o) %&gt;%\n  add_model(h2o_spec)\n\nfit_h2o &lt;- wf_h2o %&gt;%\n  fit(training(splits_x))\n\nEvaluate:\n\nmodeltime_table(fit_h2o) %&gt;%\n  modeltime_calibrate(testing(splits_x)) %&gt;%\n  modeltime_accuracy()\n\nForecast:\n\nh2o_calib &lt;- modeltime_table(fit_h2o) %&gt;%\n  modeltime_calibrate(testing(splits_x))\n\nh2o_fc &lt;- h2o_calib %&gt;%\n  modeltime_forecast(\n    new_data    = testing(splits_x),\n    actual_data = data_x\n  )\n\nh2o_fc\n\n\nh2o_fc %&gt;%\n  plot_modeltime_forecast(\n    .title = \"H2O AutoML leader vs actuals\",\n    .legend_max_width = 24\n  )\n\nConfirm the teal forecast sticks close to the orange truth without obvious look-ahead artefacts; if the line hugs every spike perfectly, double-check your feature construction for leakage.\n\n\n\n9 What H2O AutoML is doing under the hood\n\nSplits the training data internally into training/validation.\nTries many model classes: GBM, Random Forest, GLM, deep learning, stacked ensembles, etc.\nRanks them on a validation metric (e.g. RMSE or MAE).\nReturns a “leader” model that is exposed through the modeltime interface.\n\nFor deeper inspection, you can access the underlying H2O objects:\n\nleaderboard &lt;- h2o.get_leaderboard()\nleaderboard\n\n\n\n\n10 Time–series semantics & caveats\nImportant\nH2O AutoML itself does not understand time. It sees a supervised regression problem. The time–series semantics come from:\n\nHow you create features (lags, rolling stats, time signatures, promo/price variables).\nHow you split and resample data (time_series_split(), time_series_cv()).\n\nGuidelines:\n\nAlways use time–based splits for evaluation.\nPrefer to handle leakage–sensitive operations (like scaling and lagging) via recipes or timetk, not inside AutoML.\nBe cautious with automatic random CV inside H2O; keep your main validation loop in your R/rsample layer.\n\n\n\n\n11 When to use H2O AutoML vs manual models\nUse AutoML when:\n\nYou want a strong baseline quickly.\nYou’re exploring a new dataset and want to know what is achievable.\nYou’re happy with a “black–box-ish” model bundle and care more about accuracy than fine control.\n\nUse manual models (fable/modeltime) when:\n\nYou want explicit control over model class and structure (e.g. ARIMAX vs XGBoost vs DeepAR).\nInterpretability and diagnostics matter (especially in trade/revenue discussions).\nYou have custom constraints or loss functions that AutoML does not support.\n\nIn practice, a good pattern is:\n\nStart with fable models and naive baselines for sanity.\nAdd modeltime global ML models (e.g. XGBoost / LightGBM) with well–engineered features.\nUse H2O AutoML as an additional candidate in your modeltime ensembles.\nSelect the combination that works best across your backtests and business constraints.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Part IV — AutoML & H2O via modeltime.h2o</span>"
    ]
  }
]