---
title: "Part III — Extensions, retail patterns & feature engineering"
---

# Advanced timetk usage

**Motivation**

`timetk` is a powerful toolkit for time–based feature engineering, visualisation, and cross–validation. It plays very nicely with both tidymodels and direct data–frame workflows.

## Time series signatures

You can generate time–based features directly, without recipes:

```{r}
require(dplyr, quietly = TRUE)
require(ggplot2, quietly = TRUE)
time_series_split <- timetk::time_series_split
time_series_cv <- timetk::time_series_cv

xgb_spec <- parsnip::boost_tree(
  trees = 1000,
  learn_rate = 0.03,
  tree_depth = 8
) %>%
  parsnip::set_engine("xgboost") %>%
  parsnip::set_mode("regression")

data <- tibble::tibble(
  date = as.Date("2020-01-01") + 0:729,
  y    = sin(2*pi*(0:729)/7) + rnorm(730, 0, 0.2)
)

rec <- recipes::recipe(y ~ date, data = data)

data_sig <- data %>%
  timetk::tk_augment_timeseries_signature(date) %>%
  select(-tidyselect::contains("hour"), -tidyselect::contains("minute"),
         -tidyselect::contains("second"), -tidyselect::contains("am.pm"))
```

This yields fields like year, month, week, day of week, quarter, etc.

## Lags and sliding windows

```{r}
data_lags <- data_sig %>%
  timetk::tk_augment_lags(y, .lags = c(1, 7, 14))

data_roll <- data_lags %>%
  timetk::tk_augment_slidify(
    .value  = y,
    .f      = ~ mean(.x, na.rm = TRUE),
    .period = 7,
    .align  = "right",
    .partial = FALSE,
    .names  = "roll_mean_7"
  )
```

This produces lagged versions of `y` and a rolling 7–day mean.

## Visualising time–series CV plans

```{r}
cv_plan <- time_series_cv(
  data,
  assess     = 90,
  skip       = 30,
  cumulative = TRUE
)

cv_plan_tbl <- cv_plan %>%
  timetk::tk_time_series_cv_plan() %>%
  mutate(slice = factor(.id, levels = rev(unique(.id))))

ggplot(cv_plan_tbl, aes(x = date, y = slice, fill = .key)) +
  geom_tile(height = 0.6, colour = "white") +
  scale_fill_manual(values = c(training = "#2b8cbe", testing = "#f03b20")) +
  labs(
    title = "Rolling CV plan: train (blue) vs assessment (orange)",
    x = "Date",
    y = "Slice",
    fill = NULL
  ) +
  hiplot::theme_isazi(12)
```

This allows you to inspect your rolling window splits visually and confirm they make sense.

Blue blocks show the expanding training windows while orange shows the held-out horizon; scan horizontally to confirm the assessment windows march forward without gaps or overlaps.

---

# Promotions, price & cannibalisation

**Motivation**

In retail, static seasonality is the easy part. The interesting dynamics come from promotions, pricing, and interactions across products (cannibalisation). We look at classical (fable) and ML (modeltime) approaches.

We first introduced single-series promo modelling with ARIMAX in Part I, and Part II showed how to fold the same drivers into global ML models. Here we extend those ideas with richer features and cross-series context.

## ARIMAX in fable for promo & price

```{r}
data_retail <- tsibble::tsibble(
  date   = as.Date("2020-01-01") + 0:729,
  sales  = rpois(730, 100),
  promo  = rbinom(730, 1, 0.1),
  price  = runif(730, 9, 12),
  index  = date
)

fit_arimax <- data_retail %>%
  fabletools::model(
    arimax = fable::ARIMA(sales ~ promo + price)
  )

fit_arimax %>% fabletools::report()
```

Interpretation:

- Coefficient on `promo` ≈ average incremental units during promo.
- Coefficient on `price` ≈ price elasticity (units per price unit); often negative.

You can add lags and interactions, e.g. `promo + lag(promo) + price + promo:holiday_flag`.

```{r, fig.cap="Promo periods (orange) sit well above the fitted baseline, showing clean incremental lift."}
fit_arimax %>%
  fabletools::augment() %>%
  dplyr::left_join(data_retail %>% dplyr::select(date, promo), by = "date") %>%
  mutate(promo_flag = if_else(promo == 1, "Promo", "Non-promo")) %>%
  ggplot(aes(x = date)) +
  geom_line(aes(y = sales), colour = "grey70") +
  geom_line(aes(y = .fitted), colour = "#2c7fb8", linewidth = 0.6) +
  geom_point(aes(y = sales, colour = promo_flag), alpha = 0.7, size = 1.1) +
  scale_colour_manual(values = c("Promo" = "#d95f02", "Non-promo" = "#1b9e77")) +
  labs(
    title = "Promo vs non-promo fit diagnostics",
    x = "Date",
    y = "Sales",
    colour = NULL
  ) +
  hiplot::theme_isazi(12)
```

If the promo dots mingle with the baseline, consider adding lagged promos or interaction terms (promo × season) before discarding the regressor.

## ML global model for promos & price

```{r}
data_panel <- tibble(
  sku   = rep(letters[1:20], each = 730),
  date  = rep(as.Date("2020-01-01") + 0:729, 20),
  sales = rpois(20 * 730, 100),
  promo = rbinom(20 * 730, 1, 0.1),
  price = runif(20 * 730, 9, 12)
) %>%
  arrange(date, sku)

splits_p <- time_series_split(
  data_panel,
  assess     = 90,
  cumulative = TRUE
)

rec_promos <- recipes::recipe(sales ~ sku + date + promo + price,
                     data = rsample::training(splits_p)) %>%
  timetk::step_timeseries_signature(date) %>%
  recipes::step_rm(date) %>%
  recipes::step_dummy(recipes::all_nominal_predictors(), one_hot = TRUE) %>%
  recipes::step_zv(recipes::all_predictors()) %>%
  recipes::step_normalize(recipes::all_numeric_predictors())

wf_promos <- workflows::workflow() %>%
  workflows::add_recipe(rec_promos) %>%
  workflows::add_model(xgb_spec)

fit_promos <- workflows::fit(wf_promos, rsample::training(splits_p))
```

For cannibalisation, you precompute features such as:

- `category_sales_ex_sku`
- `brand_sales`
- `competitor_price`

via group–by / lag operations, then feed into the recipe.

```{r, fig.cap="Global XGBoost importance shows which levers the model leaned on first."}
require(vip, quietly = TRUE)

fit_promos %>%
  workflows::pull_workflow_fit() %>%
  vip::vip(num_features = 10, geom = "col") +
  hiplot::theme_isazi(12)
```

Promo and price should dominate; if only date-derived factors appear, revisit how you encoded the commercial drivers.

---

# Feature engineering cookbook for ML time series

**Motivation**

Global ML models live or die on features. This chapter summarises common patterns you can compose.

## Time–based features

- `step_timeseries_signature(date)` or `tk_augment_timeseries_signature()`.
- Fourier terms for long seasonalities (e.g. yearly with daily data).

## Lags & rolling statistics

In recipes:

```{r}
rec_lags <- rec %>%
  recipes::step_lag(y, lag = c(1, 7, 14)) %>%
  timetk::step_slidify(
    y,
    period = 7,
    .f = mean,
    align = "right",
    partial = FALSE,
    names = "roll_mean_7"
  ) %>%
  timetk::step_slidify(
    y,
    period = 7,
    .f = sd,
    align = "right",
    partial = FALSE,
    names = "roll_sd_7"
  )
```

These capture local dynamics that trees exploit well.

## Calendar & holiday features

Use `step_holiday()` or precomputed holiday tables to add dummies for important days, long weekends, etc.

## Price & promo–derived features

Examples:

- `discount_pct = pmax(0, (list_price - price) / list_price)`
- `promo_flag = as.integer(discount_pct > 0.1)`
- `price_index` vs category average.

## Cross–series interaction features

Use group–by / summarise to compute, for each date and category:

- `category_sales`  
- `category_sales_ex_sku`  
- `top_brand_share`  

Then join back to the main table and feed into your recipe.

```{r}
cross_feats <- data_panel %>%
  mutate(category = if_else(sku %in% letters[1:10], "A", "B")) %>%
  group_by(date, category) %>%
  mutate(
    category_sales = sum(sales),
    category_sales_ex_sku = category_sales - sales
  ) %>%
  ungroup()

cross_feats %>%
  select(sku, date, sales, category, category_sales_ex_sku) %>%
  arrange(sku, date) %>%
  head()
```

This pattern lets each SKU know the scale of its peers, unlocking cannibalisation or halo effects for downstream models.

---

# Per–SKU vs global models

**Motivation**

A key design choice: do you fit a separate model per SKU (e.g., ARIMA in fable) or a global model (e.g., XGBoost in modeltime)? It’s worth comparing them quantitatively.

## Per–SKU ARIMA (fable)

```{r}
ts_panel <- tsibble::as_tsibble(
  data_panel,
  key   = sku,
  index = date
)

split_date <- as.Date("2021-06-30")

train_ts <- ts_panel %>%
  filter(date <= split_date)

test_ts <- ts_panel %>%
  filter(date > split_date)

fit_per <- train_ts %>%
  fabletools::model(arima = fable::ARIMA(sales))

acc_per <- fc_per <- fit_per %>%
  fabletools::forecast(h = dplyr::n_distinct(test_ts$date)) %>%
  as_tibble() %>%
  transmute(
    sku,
    date,
    .pred = purrr::map_dbl(sales, mean)
  ) %>%
  left_join(
    tsibble::as_tibble(test_ts) %>%
      select(sku, date, sales),
    by = c("sku", "date")
  ) %>%
  group_by(sku) %>%
  summarise(
    rmse_per = yardstick::rmse_vec(truth = sales, estimate = .pred),
    .groups = "drop"
  )
```

## Global XGBoost (modeltime)

```{r}
splits_panel <- time_series_split(
  data_panel,
  assess     = 600,
  cumulative = TRUE
)

rec_global <- recipes::recipe(sales ~ sku + date + promo + price,
                     data = rsample::training(splits_panel)) %>%
  timetk::step_timeseries_signature(date) %>%
  recipes::step_rm(date) %>%
  recipes::step_dummy(recipes::all_nominal_predictors(), one_hot = TRUE) %>%
  recipes::step_zv(recipes::all_predictors()) %>%
  recipes::step_normalize(recipes::all_numeric_predictors())

wf_global <- workflows::workflow() %>%
  workflows::add_recipe(rec_global) %>%
  workflows::add_model(xgb_spec)

fit_global <- workflows::fit(wf_global, rsample::training(splits_panel))

fc_global <- modeltime::modeltime_table(fit_global) %>%
  modeltime::modeltime_calibrate(rsample::testing(splits_panel)) %>%
  modeltime::modeltime_forecast(
    new_data    = rsample::testing(splits_panel),
    actual_data = data_panel,
    keep_data   = TRUE
  )
```

Map to per–SKU RMSE:

```{r}
acc_global <- fc_global %>%
  filter(.key == "prediction") %>%
  group_by(sku) %>%
  summarise(
    rmse_global = yardstick::rmse_vec(truth = sales, estimate = .value),
    .groups = "drop"
  )

comparison <- acc_per %>%
  left_join(acc_global, by = "sku") %>%
  mutate(diff = rmse_per - rmse_global)
```

You can now see how many SKUs prefer global vs per–SKU models and by how much.

---

# End–to–end engine pattern

**Motivation**

In production you want a repeatable pipeline: ingest data, build features, train models, backtest, and store configuration for deployment.

A minimal pattern:

1. **Process tables** (facts + dimensions).
2. **Feature builder** function.
3. **Modelling** function(s) (fable and/or modeltime).
4. **Backtest harness** (loop over cutoff dates).
5. **Model registry** (chosen config per segment).

Pseudocode for modelling step:

```{r}
fit_forecast_models <- function(features, horizon, engine = c("fable", "modeltime")) {
  engine <- match.arg(engine)
  if (engine == "fable") {
    ts <- features %>%
      tsibble::as_tsibble(key = c(sku, depot), index = date)

    ts %>% fabletools::model(fable::ARIMA(qty))
  } else {
    splits <- time_series_split(features, assess = horizon, cumulative = TRUE)

    rec <- recipes::recipe(qty ~ ., data = rsample::training(splits)) %>%
      timetk::step_timeseries_signature(date) %>%
      recipes::step_rm(date) %>%
      recipes::step_dummy(recipes::all_nominal_predictors()) %>%
      recipes::step_zv(recipes::all_predictors()) %>%
      recipes::step_normalize(recipes::all_numeric_predictors())

    wf <- workflows::workflow() %>%
      workflows::add_recipe(rec) %>%
      workflows::add_model(xgb_spec)

    workflows::fit(wf, rsample::training(splits))
  }
}
```

You then wrap this in backtesting and orchestration appropriate to your environment (e.g., cron, Airflow, Hudson–style process orchestrator).
