---
title: "Part III — Extensions, retail patterns & feature engineering"
---

# 19. Advanced timetk usage

**Motivation**

`timetk` is a powerful toolkit for time–based feature engineering, visualisation, and cross–validation. It plays very nicely with both tidymodels and direct data–frame workflows.

## 19.1 Time series signatures

You can generate time–based features directly, without recipes:

```r
library(timetk)

data_sig <- data %>%
  tk_augment_timeseries_signature(date) %>%
  select(-contains("hour"), -contains("minute"),
         -contains("second"), -contains("am.pm"))
```

This yields fields like year, month, week, day of week, quarter, etc.

## 19.2 Lags and sliding windows

```r
data_lags <- data_sig %>%
  tk_augment_lags(y, .lags = c(1, 7, 14))

data_roll <- data_lags %>%
  tk_augment_slidify(
    .value  = y,
    .f      = ~ mean(.x, na.rm = TRUE),
    .period = 7,
    .align  = "right",
    .partial = FALSE,
    .names  = "roll_mean_7"
  )
```

This produces lagged versions of `y` and a rolling 7–day mean.

## 19.3 Visualising time–series CV plans

```r
cv_plan <- time_series_cv(
  data,
  assess     = 90,
  skip       = 30,
  cumulative = TRUE
)

cv_plan %>%
  tk_time_series_cv_plan() %>%
  plot_time_series_cv_plan(
    .date_var = date,
    .value    = y
  )
```

This allows you to inspect your rolling window splits visually and confirm they make sense.

---

# 20. Promotions, price & cannibalisation

**Motivation**

In retail, static seasonality is the easy part. The interesting dynamics come from promotions, pricing, and interactions across products (cannibalisation). We look at classical (fable) and ML (modeltime) approaches.

## 20.1 ARIMAX in fable for promo & price

```r
data_retail <- tsibble(
  date   = as.Date("2020-01-01") + 0:729,
  sales  = rpois(730, 100),
  promo  = rbinom(730, 1, 0.1),
  price  = runif(730, 9, 12),
  index  = date
)

fit_arimax <- data_retail %>%
  model(
    arimax = ARIMA(sales ~ promo + price)
  )

fit_arimax %>% report()
```

Interpretation:

- Coefficient on `promo` ≈ average incremental units during promo.
- Coefficient on `price` ≈ price elasticity (units per price unit); often negative.

You can add lags and interactions, e.g. `promo + lag(promo) + price + promo:holiday_flag`.

## 20.2 ML global model for promos & price

```r
data_panel <- tibble(
  sku   = rep(letters[1:20], each = 730),
  date  = rep(as.Date("2020-01-01") + 0:729, 20),
  sales = rpois(20 * 730, 100),
  promo = rbinom(20 * 730, 1, 0.1),
  price = runif(20 * 730, 9, 12)
)

splits_p <- time_series_split(
  data_panel,
  assess     = 90,
  cumulative = TRUE
)

rec_promos <- recipe(sales ~ sku + date + promo + price,
                     data = training(splits_p)) %>%
  step_timeseries_signature(date) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

wf_promos <- workflow() %>%
  add_recipe(rec_promos) %>%
  add_model(xgb_spec)

fit_promos <- fit(wf_promos, training(splits_p))
```

For cannibalisation, you precompute features such as:

- `category_sales_ex_sku`
- `brand_sales`
- `competitor_price`

via group–by / lag operations, then feed into the recipe.

---

# 21. Feature engineering cookbook for ML time series

**Motivation**

Global ML models live or die on features. This chapter summarises common patterns you can compose.

## 21.1 Time–based features

- `step_timeseries_signature(date)` or `tk_augment_timeseries_signature()`.
- Fourier terms for long seasonalities (e.g. yearly with daily data).

## 21.2 Lags & rolling statistics

In recipes:

```r
rec_lags <- rec %>%
  step_lag(y, lag = c(1, 7, 14)) %>%
  step_roll_mean(y, lag = 7, window = 7,
                 align = "right", id = "roll_mean_7") %>%
  step_roll_sd(y, lag = 7, window = 7,
               align = "right", id = "roll_sd_7")
```

These capture local dynamics that trees exploit well.

## 21.3 Calendar & holiday features

Use `step_holiday()` or precomputed holiday tables to add dummies for important days, long weekends, etc.

## 21.4 Price & promo–derived features

Examples:

- `discount_pct = pmax(0, (list_price - price) / list_price)`
- `promo_flag = as.integer(discount_pct > 0.1)`
- `price_index` vs category average.

## 21.5 Cross–series interaction features

Use group–by / summarise to compute, for each date and category:

- `category_sales`  
- `category_sales_ex_sku`  
- `top_brand_share`  

Then join back to the main table and feed into your recipe.

---

# 22. Per–SKU vs global models

**Motivation**

A key design choice: do you fit a separate model per SKU (e.g., ARIMA in fable) or a global model (e.g., XGBoost in modeltime)? It’s worth comparing them quantitatively.

## 22.1 Per–SKU ARIMA (fable)

```r
ts_panel <- as_tsibble(
  data_panel,
  key   = sku,
  index = date
)

split_date <- as.Date("2021-12-31")

train_ts <- ts_panel %>%
  filter(date <= split_date)

test_ts <- ts_panel %>%
  filter(date > split_date)

fit_per <- train_ts %>%
  model(arima = ARIMA(sales))

fc_per <- fit_per %>%
  forecast(h = n_distinct(test_ts$date))

acc_per <- fc_per %>%
  accuracy(test_ts, by = "sku") %>%
  select(sku, rmse_per = RMSE)
```

## 22.2 Global XGBoost (modeltime)

```r
splits_panel <- time_series_split(
  data_panel,
  assess     = sum(data_panel$date > split_date),
  cumulative = TRUE
)

rec_global <- recipe(sales ~ sku + date + promo + price,
                     data = training(splits_panel)) %>%
  step_timeseries_signature(date) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

wf_global <- workflow() %>%
  add_recipe(rec_global) %>%
  add_model(xgb_spec)

fit_global <- fit(wf_global, training(splits_panel))

fc_global <- modeltime_table(fit_global) %>%
  modeltime_calibrate(testing(splits_panel)) %>%
  modeltime_forecast(
    new_data    = testing(splits_panel),
    actual_data = data_panel
  )
```

Map to per–SKU RMSE:

```r
library(dplyr)
library(yardstick)

acc_global <- fc_global %>%
  group_by(sku) %>%
  summarise(
    rmse_global = rmse_vec(truth = .value, estimate = .pred)
  )

comparison <- acc_per %>%
  left_join(acc_global, by = "sku") %>%
  mutate(diff = rmse_per - rmse_global)
```

You can now see how many SKUs prefer global vs per–SKU models and by how much.

---

# 23. End–to–end engine pattern

**Motivation**

In production you want a repeatable pipeline: ingest data, build features, train models, backtest, and store configuration for deployment.

A minimal pattern:

1. **Process tables** (facts + dimensions).
2. **Feature builder** function.
3. **Modelling** function(s) (fable and/or modeltime).
4. **Backtest harness** (loop over cutoff dates).
5. **Model registry** (chosen config per segment).

Pseudocode for modelling step:

```r
fit_forecast_models <- function(features, horizon, engine = c("fable", "modeltime")) {
  engine <- match.arg(engine)
  if (engine == "fable") {
    ts <- features %>%
      as_tsibble(key = c(sku, depot), index = date)

    ts %>% model(ARIMA(qty))
  } else {
    splits <- time_series_split(features, assess = horizon, cumulative = TRUE)

    rec <- recipe(qty ~ ., data = training(splits)) %>%
      step_timeseries_signature(date) %>%
      step_dummy(all_nominal_predictors()) %>%
      step_normalize(all_numeric_predictors())

    wf <- workflow() %>%
      add_recipe(rec) %>%
      add_model(xgb_spec)

    fit(wf, training(splits))
  }
}
```

You then wrap this in backtesting and orchestration appropriate to your environment (e.g., cron, Airflow, Hudson–style process orchestrator).
