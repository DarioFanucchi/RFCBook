---
title: "Part II — Forecasting with modeltime & tidymodels"
---

# Time series splits & the ML mindset

**Motivation**

Machine–learning–style forecasting treats time series as supervised learning with strong temporal structure. The critical decision is the *evaluation protocol*; you must respect time when you split data.

```{r}
require(dplyr, quietly=TRUE)
require(modeltime, quietly=TRUE)
time_series_split <- timetk::time_series_split
time_series_cv <- timetk::time_series_cv

data <- tibble::tibble(
  date  = as.Date("2020-01-01") + 0:729,
  y     = sin(2*pi*(0:729)/7) + 2*sin(2*pi*(0:729)/30.5) + sin(2*pi*(0:729)/365) + rnorm(730, 0, 0.2), # Sample data with weekly, monthly, yearly trends
)

splits <- time_series_split(
  data,
  assess     = 90,
  cumulative = TRUE
)
```

- `training(splits)` gives history up to the cutoff.
- `testing(splits)` gives the final 90 days for evaluation.

---

# A basic XGBoost forecast

**Motivation**

Gradient–boosted trees (XGBoost, LightGBM, CatBoost) are state of the art in many retail forecasting benchmarks when combined with good feature engineering.

```{r}
rec <- recipes::recipe(y ~ date, data = rsample::training(splits)) %>%
  timetk::step_timeseries_signature(date) %>%
  recipes::step_rm(
    date,
    tidyselect::contains("hour"),
    tidyselect::contains("minute"),
    tidyselect::contains("second"),
    tidyselect::contains("am.pm"),
    tidyselect::contains("lbl")
  ) %>%
  recipes::step_zv(recipes::all_predictors()) %>%
  recipes::step_normalize(recipes::all_numeric_predictors())

xgb_spec <- parsnip::boost_tree(
  trees      = 2000,
  learn_rate = 0.03,
  tree_depth = 8
) %>%
  parsnip::set_engine("xgboost") %>%
  parsnip::set_mode("regression")

wf_xgb <- workflows::workflow() %>%
  workflows::add_recipe(rec) %>%
  workflows::add_model(xgb_spec)

fit_xgb <- workflows::fit(wf_xgb, rsample::training(splits))
```

Forecast and evaluate:

```{r}
xgb_tbl <- modeltime::modeltime_table(fit_xgb)

xgb_calib <- xgb_tbl %>%
  modeltime::modeltime_calibrate(rsample::testing(splits))

xgb_fc <- xgb_calib %>%
  modeltime::modeltime_forecast(
    new_data    = rsample::testing(splits),
    actual_data = data
  )

xgb_fc
```

```{r, fig.cap="Holdout forecast from the basic XGBoost workflow."}
xgb_fc %>%
  modeltime::plot_modeltime_forecast(
    .title = "XGBoost forecast vs actuals",
    .legend_max_width = 24
  )
```

Look for systematic under/over prediction across the holdout—if the orange forecast line consistently lags the truth, feed more lags or external drivers before chasing hyperparameters.

---

# Adding a classical ARIMA baseline

**Motivation**

Even when you believe ML will win, you should verify it beats a decent statistical model.

```{r}
rec_arima <- recipes::recipe(y ~ date, data = rsample::training(splits))

fit_arima <- workflows::workflow() %>%
  workflows::add_recipe(rec_arima) %>%
  workflows::add_model(
    modeltime::arima_reg() %>%
      parsnip::set_engine("auto_arima")
  ) %>%
  workflows::fit(rsample::training(splits))

model_tbl <- modeltime::modeltime_table(
  fit_xgb,
  fit_arima
)

accuracy_tbl <- model_tbl %>%
  modeltime::modeltime_calibrate(rsample::testing(splits)) %>%
  modeltime::modeltime_accuracy()

accuracy_tbl
```

```{r, fig.cap="ARIMA offers a sturdy baseline; XGBoost wins only if the engineered features add signal."}
accuracy_tbl %>%
  ggplot2::ggplot(ggplot2::aes(x = reorder(.model_desc, rmse), y = rmse)) +
  ggplot2::geom_col(fill = "#3182bd") +
  ggplot2::coord_flip() +
  ggplot2::labs(
    x = NULL,
    y = "RMSE",
    title = "Holdout accuracy by model"
  ) +
  hiplot::theme_isazi(12)
```

You now have side–by–side accuracy for ML and classical methods.

Even if RMSE numbers look close, the bar plot highlights whether the ML lift is material enough to justify the extra complexity.

---

# Exogenous regressors: promos, prices, features

**Motivation**

Machine–learning models handle many regressors naturally. This is key for retail and supply–chain: promotions, prices, macro variables, calendar features, etc.

```{r}
set.seed(1)

data_x <- data %>%
  mutate(
    promo = rbinom(n(), 1, 0.1),
    price = runif(n(), 9, 12)
  )

splits_x <- time_series_split(
  data_x,
  assess     = 90,
  cumulative = TRUE
)

rec_x <- recipes::recipe(y ~ ., data = rsample::training(splits_x)) %>%
  timetk::step_timeseries_signature(date) %>%
  recipes::step_rm(
    date,
    tidyselect::contains("hour"),
    tidyselect::contains("minute"),
    tidyselect::contains("second"),
    tidyselect::contains("am.pm"),
    tidyselect::contains("lbl")
  ) %>%
  recipes::step_zv(recipes::all_predictors()) %>%
  recipes::step_normalize(recipes::all_numeric_predictors())

wf_xgb_x <- workflows::workflow() %>%
  workflows::add_recipe(rec_x) %>%
  workflows::add_model(xgb_spec)

fit_xgb_x <- workflows::fit(wf_xgb_x, rsample::training(splits_x))
```

You can interpret feature importance with packages like `vip`, `DALEX`, or `iml`.

Use this when you want flexible, global promo/price effects; if you need per-series coefficients, see the ARIMAX treatment in Part I, and for cross-series engineered drivers jump ahead to Part III.

```{r, fig.cap="Promo + price features bend the forecast line upward when uplift signals appear in the training data."}
modeltime::modeltime_table(fit_xgb_x) %>%
  modeltime::modeltime_calibrate(rsample::testing(splits_x)) %>%
  modeltime::modeltime_forecast(
    new_data    = rsample::testing(splits_x),
    actual_data = data_x
  ) %>%
  modeltime::plot_modeltime_forecast(
    .title = "Holdout forecast with promo/price features",
    .legend_max_width = 24
  )
```

Notice whether spikes during promo periods are now captured; if not, increase lag depth or add richer promo descriptors.

---

# Time–aware hyperparameter tuning

**Motivation**

If you tune XGBoost with random cross–validation, you leak future information. Instead, use time–series CV (rolling origin) for tuning.

```{r}
cv_folds <- time_series_cv(
  data_x,
  assess     = 90,
  skip       = 30,
  cumulative = TRUE
)

tuned <- tune::tune_grid(
  wf_xgb_x,
  resamples = cv_folds,
  grid      = 20,
  control   = tune::control_grid(verbose = FALSE)
)

best <- tune::select_best(tuned, metric = "rmse")

final_wf <- tune::finalize_workflow(wf_xgb_x, best)
fit_final <- workflows::fit(final_wf, data_x)
```

The tuned workflow can then be used in `modeltime_table()` and productionised.

---

# Ensembles in modeltime

**Motivation**

Ensembling multiple models (ML + classical) often yields robustness and extra accuracy at very low marginal cost.

```{r}
model_tbl <- modeltime::modeltime_table(
  fit_xgb_x,
  fit_arima
)

ensemble <- model_tbl %>%
  modeltime.ensemble::ensemble_average(type = "mean")

ensemble %>%
  modeltime::modeltime_calibrate(rsample::testing(splits_x)) %>%
  modeltime::modeltime_accuracy()

ensemble_fc <- ensemble %>%
  modeltime::modeltime_calibrate(rsample::testing(splits_x)) %>%
  modeltime::modeltime_forecast(
    new_data    = rsample::testing(splits_x),
    actual_data = data_x
  )
```

```{r, fig.cap="Simple mean ensemble of ML + ARIMA across the holdout window."}
ensemble_fc %>%
  modeltime::plot_modeltime_forecast(
    .title = "Ensemble forecast vs truth",
    .legend_max_width = 24
  )
```

You can also use weighted ensembles or meta–learners via `stacks`.

The blended line often inherits the best bits of each contributor; if it still wobbles, inspect residual correlations to design smarter weighting schemes.

---

# Global models across many series

**Motivation**

Global models train one model across many series (e.g. SKUs, depots, stores), often outperforming per–series models when data is sparse.

```{r}
data_panel <- tibble::tibble(
  sku   = rep(letters[1:20], each = 730),
  date  = rep(as.Date("2020-01-01") + 0:729, 20),
  y     = rnorm(20 * 730) + as.numeric(as.factor(sku)) * 0.2,
  promo = rbinom(20 * 730, 1, 0.1),
  price = runif(20 * 730, 9, 12)
) %>%
  arrange(date, sku)

splits_panel <- time_series_split(
  data_panel,
  assess     = 90,
  cumulative = TRUE
)

rec_panel <- recipes::recipe(y ~ sku + date + promo + price,
                    data = rsample::training(splits_panel)) %>%
  timetk::step_timeseries_signature(date) %>%
  recipes::step_rm(date) %>%
  recipes::step_dummy(recipes::all_nominal_predictors()) %>%
  recipes::step_zv(recipes::all_predictors()) %>%
  recipes::step_normalize(recipes::all_numeric_predictors())

wf_global <- workflows::workflow() %>%
  workflows::add_recipe(rec_panel) %>%
  workflows::add_model(xgb_spec)

fit_global <- workflows::fit(wf_global, rsample::training(splits_panel))
```

This XGBoost model learns patterns shared across SKUs and depots.

---

# Prophet in modeltime

**Motivation**

Prophet is a popular model for business time series with strong seasonal patterns and holiday effects. `modeltime` wraps it in the same interface as your other models.

```{r}
require(prophet, quietly=TRUE)

prop_spec <- modeltime::prophet_reg(
  seasonality_yearly = TRUE,
  seasonality_weekly = TRUE,
  seasonality_daily  = FALSE,
  changepoint_num    = 25,
  changepoint_range  = 0.9
) %>%
  parsnip::set_engine("prophet")

rec_prophet <- recipes::recipe(y ~ date, data = rsample::training(splits))  # Prophet needs date and y

wf_prophet <- workflows::workflow() %>%
  workflows::add_recipe(rec_prophet) %>%
  workflows::add_model(prop_spec)

fit_prophet <- workflows::fit(wf_prophet, rsample::training(splits))
```

Prophet with extra regressors:

```{r}
rec_prophet_x <- recipes::recipe(y ~ date + promo + price,
                        data = rsample::training(splits_x)) %>%
  recipes::step_mutate(
    promo = as.numeric(promo),
    price = price
  )

fit_prophet_x <- workflows::workflow() %>%
  workflows::add_recipe(rec_prophet_x) %>%
  workflows::add_model(prop_spec) %>%
  workflows::fit(rsample::training(splits_x))
```

Boosted Prophet (`prophet_boost()`) adds a gradient–boosting component on Prophet’s residuals; the interface is analogous, with `set_engine("prophet_xgboost")`.
